{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1:Computes naïve proportion of simple sentences\n",
        "\n",
        "Step 2: Builds a simple classifier on clean subsets\n",
        "\n",
        "Step 3: Uses it to estimate an adjusted true proportion\n",
        "\n",
        "Step 4: Detects Vikidia-style simple sentences inside the complex-labelled set\n",
        "\n",
        "Step 5: Prints + saves a small summary\n",
        "\n",
        "schema :Label = 1(simple), Label = 0(complex)"
      ],
      "metadata": {
        "id": "an0t1C9FmcuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9LGyuKcmR6N",
        "outputId": "72409709-5086-4b17-ef6f-2ccef08d494e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezSiMbvNnDK9",
        "outputId": "0469dd7e-2b8c-4077-cc40-ad12253077b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "0BJoQKFTnQDY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR_DEFAULT = Path(\"/content/drive/MyDrive/idem-candidate-task-data/data\")\n",
        "OUTPUT_DIR_DEFAULT = Path(\"/content/drive/MyDrive/idem-candidate-task-data/data\")\n",
        "\n",
        "LANG_FILES = {\n",
        "    \"en\": \"En-Dataset.csv\",\n",
        "    \"fr\": \"Fr-Dataset.csv\",\n",
        "}\n",
        "\n",
        "EXPECTED_COLUMNS = [\n",
        "    \"ID\",\n",
        "    \"Name\",\n",
        "    \"Sentence\",\n",
        "    \"Label\",\n",
        "    \"LengthWords\",\n",
        "    \"LengthChars\",\n",
        "]\n",
        "\n",
        "\n",
        "def load_dataset(data_dir: Path, lang: str) -> pd.DataFrame:\n",
        "    \"\"\"Load and validate dataset for a given language ('en' or 'fr').\"\"\"\n",
        "    fname = LANG_FILES[lang]\n",
        "    path = data_dir / fname\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Expected file not found: {path}\")\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    missing = set(EXPECTED_COLUMNS) - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"{path} is missing expected columns: {missing}\")\n",
        "\n",
        "    # Enforce column order for consistency\n",
        "    df = df[EXPECTED_COLUMNS]\n",
        "    return df"
      ],
      "metadata": {
        "id": "wGQz1LulnYhx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_clean_subsets(\n",
        "    df: pd.DataFrame,\n",
        "    min_per_class: int = 200,\n",
        ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Define high-confidence \"clean\" subsets for simple and complex.\n",
        "\n",
        "    Heuristic:\n",
        "    - Confident simple: Label == 1 and short sentences\n",
        "    - Confident complex: Label == 0 and long sentences\n",
        "\n",
        "    If those conditions do not give enough examples, fall back to\n",
        "    using quartiles of LengthWords.\n",
        "    \"\"\"\n",
        "    # First try fixed thresholds\n",
        "    simple_max_len = 10\n",
        "    complex_min_len = 20\n",
        "\n",
        "    clean_simple = df[(df[\"Label\"] == 1) & (df[\"LengthWords\"] <= simple_max_len)]\n",
        "    clean_complex = df[(df[\"Label\"] == 0) & (df[\"LengthWords\"] >= complex_min_len)]\n",
        "\n",
        "    if len(clean_simple) < min_per_class or len(clean_complex) < min_per_class:\n",
        "        # Fall back to quantile-based thresholds to get enough data\n",
        "        q1 = df[\"LengthWords\"].quantile(0.25)\n",
        "        q3 = df[\"LengthWords\"].quantile(0.75)\n",
        "        clean_simple = df[(df[\"Label\"] == 1) & (df[\"LengthWords\"] <= q1)]\n",
        "        clean_complex = df[(df[\"Label\"] == 0) & (df[\"LengthWords\"] >= q3)]\n",
        "\n",
        "    return clean_simple, clean_complex"
      ],
      "metadata": {
        "id": "jHpwmDOOoFXa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compute the naive estimate"
      ],
      "metadata": {
        "id": "wTJqsz2sBT-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_simple_classifier(\n",
        "    clean_simple: pd.DataFrame,\n",
        "    clean_complex: pd.DataFrame,\n",
        "    max_features: int = 20000,\n",
        ") -> tuple[TfidfVectorizer, LogisticRegression]:\n",
        "    \"\"\"\n",
        "    Train TF-IDF + Logistic Regression classifier on clean examples.\n",
        "\n",
        "    Target labels:\n",
        "    - y = 1 for simple (Label == 1)\n",
        "    - y = 0 for complex (Label == 0)\n",
        "    \"\"\"\n",
        "    clean_df = pd.concat([clean_simple, clean_complex], ignore_index=True)\n",
        "    y = clean_df[\"Label\"].astype(int)  # 1 = simple, 0 = complex\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=max_features,\n",
        "        ngram_range=(1, 2),\n",
        "        lowercase=True,\n",
        "    )\n",
        "    X = vectorizer.fit_transform(clean_df[\"Sentence\"])\n",
        "\n",
        "    clf = LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        class_weight=\"balanced\",\n",
        "    )\n",
        "    clf.fit(X, y)\n",
        "    return vectorizer, clf"
      ],
      "metadata": {
        "id": "4tbMV_vHoIhl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Design a better estimation method, using any reasonable approach."
      ],
      "metadata": {
        "id": "hij4DBaOBZn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_vikidia_like(\n",
        "    df: pd.DataFrame,\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    p_simple_col: str = \"p_simple\",\n",
        "    label_col: str = \"Label\",\n",
        "    simple_label: int = 1,\n",
        "    complex_label: int = 0,\n",
        "    top_k_simple: int = 2000,\n",
        "    similarity_quantile: float = 0.95,\n",
        ") -> tuple[float, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Estimate the proportion of Vikidia-style simple sentences\n",
        "    among complex-labelled sentences.\n",
        "\n",
        "    Operationalisation:\n",
        "    - Take top_k_simple sentences with highest p_simple as\n",
        "      prototypical simple sentences.\n",
        "    - Represent sentences using the same TF-IDF vectorizer.\n",
        "    - For each complex-labelled sentence, compute maximum\n",
        "      cosine similarity to these prototypical simple ones.\n",
        "    - Mark as Vikidia-like if the maximum similarity is above\n",
        "      a high quantile threshold (e.g. 95th percentile).\n",
        "    \"\"\"\n",
        "    df_sorted = df.sort_values(p_simple_col, ascending=False)\n",
        "    ref_simple = df_sorted.head(top_k_simple)\n",
        "\n",
        "    X_ref = vectorizer.transform(ref_simple[\"Sentence\"])\n",
        "\n",
        "    complex_mask = df[label_col] == complex_label\n",
        "    df_complex = df[complex_mask]\n",
        "\n",
        "    if df_complex.empty:\n",
        "        return 0.0, np.zeros(len(df), dtype=bool)\n",
        "\n",
        "    X_complex = vectorizer.transform(df_complex[\"Sentence\"])\n",
        "\n",
        "    sims = cosine_similarity(X_complex, X_ref)\n",
        "    max_sim = sims.max(axis=1)\n",
        "\n",
        "    # Threshold picked adaptively based on similarity distribution\n",
        "    threshold = np.quantile(max_sim, similarity_quantile)\n",
        "    vikidia_like_complex = max_sim >= threshold\n",
        "\n",
        "    vikidia_like_mask = np.zeros(len(df), dtype=bool)\n",
        "    vikidia_like_mask[df_complex.index.values] = vikidia_like_complex\n",
        "\n",
        "    prop_vikidia_like_in_complex = (\n",
        "        vikidia_like_complex.mean() if len(vikidia_like_complex) > 0 else 0.0\n",
        "    )\n",
        "    return prop_vikidia_like_in_complex, vikidia_like_mask\n"
      ],
      "metadata": {
        "id": "qcubaPWxoL4A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Produce final estimates"
      ],
      "metadata": {
        "id": "hbG2pYn6BfaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyse_language(\n",
        "    lang: str,\n",
        "    data_dir: Path,\n",
        "    output_dir: Path,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Full pipeline for one language:\n",
        "    - Load data\n",
        "    - Compute naive proportion of simple sentences\n",
        "    - Train classifier on clean subset\n",
        "    - Compute adjusted proportion of simple sentences\n",
        "    - Estimate Vikidia-like simple sentences among complex-labelled\n",
        "    - Save enriched dataset and return summary metrics\n",
        "    \"\"\"\n",
        "    df = load_dataset(data_dir, lang)\n",
        "\n",
        "    print(f\"\\n{lang.upper()} Dataset – Task 1 Analysis\")\n",
        "\n",
        "    # 1. Naive estimate of simple proportion\n",
        "    naive_prop_simple = (df[\"Label\"] == 1).mean()\n",
        "    print(f\"Naïve simple proportion (Label == 1): {naive_prop_simple:.4f}\")\n",
        "\n",
        "    # 2. Clean subsets for training\n",
        "    clean_simple, clean_complex = select_clean_subsets(df)\n",
        "    print(\n",
        "        f\"Clean subset sizes: simple={len(clean_simple)}, complex={len(clean_complex)}\"\n",
        "    )\n",
        "\n",
        "    # 3. Train classifier & get probabilities\n",
        "    vectorizer, clf = train_simple_classifier(clean_simple, clean_complex)\n",
        "\n",
        "    X_all = vectorizer.transform(df[\"Sentence\"])\n",
        "    p_simple = clf.predict_proba(X_all)[:, 1]  # probability of being simple\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"p_simple\"] = p_simple\n",
        "\n",
        "    adjusted_prop_simple = float(df[\"p_simple\"].mean())\n",
        "    print(f\"Adjusted simple proportion (mean p_simple): {adjusted_prop_simple:.4f}\")\n",
        "\n",
        "    # 4. Vikidia-style sentences among complex-labelled ones\n",
        "    prop_vikidia_like_in_complex, vikidia_like_mask = estimate_vikidia_like(\n",
        "        df,\n",
        "        vectorizer,\n",
        "        p_simple_col=\"p_simple\",\n",
        "        label_col=\"Label\",\n",
        "        simple_label=1,\n",
        "        complex_label=0,\n",
        "        top_k_simple=2000,\n",
        "        similarity_quantile=0.95,\n",
        "    )\n",
        "    df[\"vikidia_like_in_complex\"] = vikidia_like_mask\n",
        "\n",
        "    print(\n",
        "        f\"Proportion of complex-labelled sentences that look Vikidia-style simple: \"\n",
        "        f\"{prop_vikidia_like_in_complex:.4f}\"\n",
        "    )\n",
        "\n",
        "    # 5. Save enriched dataset for inspection\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    out_path = output_dir / f\"{lang}_with_probs_and_vikidia_like.csv\"\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"Enriched dataset saved to: {out_path}\")\n",
        "\n",
        "    summary = {\n",
        "        \"language\": lang,\n",
        "        \"n_sentences\": len(df),\n",
        "        \"naive_prop_simple\": naive_prop_simple,\n",
        "        \"adjusted_prop_simple\": adjusted_prop_simple,\n",
        "        \"prop_vikidia_like_in_complex\": prop_vikidia_like_in_complex,\n",
        "    }\n",
        "    return summary\n",
        "\n"
      ],
      "metadata": {
        "id": "qCtirqltoQOH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(data_dir: str = DATA_DIR_DEFAULT, output_dir: str = OUTPUT_DIR_DEFAULT):\n",
        "    data_dir = Path(data_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "\n",
        "    summaries = []\n",
        "    for lang in (\"en\", \"fr\"):\n",
        "        summary = analyse_language(lang, data_dir, output_dir)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    summary_df = pd.DataFrame(summaries)\n",
        "\n",
        "    print(\"\\nFinal Summary\")\n",
        "    with pd.option_context(\"display.max_columns\", None):\n",
        "        print(\n",
        "            summary_df.to_string(\n",
        "                index=False,\n",
        "                float_format=lambda x: f\"{x:.4f}\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Task 1 — Estimate true proportion of simple sentences.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--data-dir\",\n",
        "        type=str,\n",
        "        default=str(DATA_DIR_DEFAULT),\n",
        "        help=\"Directory containing En-Dataset.csv and Fr-Dataset.csv\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output-dir\",\n",
        "        type=str,\n",
        "        default=str(OUTPUT_DIR_DEFAULT),\n",
        "        help=\"Directory to save enriched datasets and summary CSV.\",\n",
        "    )\n",
        "    args = parser.parse_args([]) # Pass an empty list to parse_args()\n",
        "    main(data_dir=args.data_dir, output_dir=args.output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMfknHRgoUCL",
        "outputId": "e8dc7dfd-5530-46a2-9e4b-6bc5bfe127ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EN Dataset – Task 1 Analysis\n",
            "Naïve simple proportion (Label == 1): 0.9382\n",
            "Clean subset sizes: simple=8955, complex=6166\n",
            "Adjusted simple proportion (mean p_simple): 0.3770\n",
            "Proportion of complex-labelled sentences that look Vikidia-style simple: 0.0500\n",
            "Enriched dataset saved to: /content/drive/MyDrive/idem-candidate-task-data/data/en_with_probs_and_vikidia_like.csv\n",
            "\n",
            "FR Dataset – Task 1 Analysis\n",
            "Naïve simple proportion (Label == 1): 0.8656\n",
            "Clean subset sizes: simple=60039, complex=102665\n",
            "Adjusted simple proportion (mean p_simple): 0.3690\n",
            "Proportion of complex-labelled sentences that look Vikidia-style simple: 0.0500\n",
            "Enriched dataset saved to: /content/drive/MyDrive/idem-candidate-task-data/data/fr_with_probs_and_vikidia_like.csv\n",
            "\n",
            "Final Summary\n",
            "language  n_sentences  naive_prop_simple  adjusted_prop_simple  prop_vikidia_like_in_complex\n",
            "      en       290708             0.9382                0.3770                        0.0500\n",
            "      fr      1699063             0.8656                0.3690                        0.0500\n"
          ]
        }
      ]
    }
  ]
}